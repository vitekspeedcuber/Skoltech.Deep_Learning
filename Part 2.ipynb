{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "gFe273vKFgF-"
   },
   "source": [
    "# Homework 2, *part 2*\n",
    "### (60 points total)\n",
    "\n",
    "In this part, you will build a convolutional neural network (CNN) to solve (yet another) image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n",
    "\n",
    "**Unlike part 1**, you are now free to use the full power of PyTorch and its subpackages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKjuxXAgFgGA"
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "* This file.\n",
    "* A \"checkpoint file\" `\"checkpoint.pth\"` that contains your CNN's weights (you get them from `model.state_dict()`). Obtain it with `torch.save(..., \"checkpoint.pth\")`. When grading, we will load it to evaluate your accuracy.\n",
    "\n",
    "**Should you decide to put your `\"checkpoint.pth\"` on Google Drive, update (edit) the following cell with the link to it:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4kOWlcXFgGD"
   },
   "source": [
    "### [Dear TAs, I've put my \"checkpoint.pth\" on Google Drive, download it here](http://your-link-here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S-WyYK8FgGF"
   },
   "source": [
    "## Grading\n",
    "\n",
    "* 9 points for reproducible training code and a filled report below.\n",
    "* 11 points for building a network that gets above 25% accuracy.\n",
    "* 4 points for using an **interactive** (please don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example Tensorboard ([with this library](https://github.com/lanpa/tensorboardX) and [an extra hack for Colab](https://stackoverflow.com/a/57791702)). In this notebook, insert screenshots of accuracy and loss plots (training and validation) over iterations/epochs/time.\n",
    "* 6 points for beating each of these accuracy milestones on the private **test** set:\n",
    "  * 30%\n",
    "  * 34%\n",
    "  * 38%\n",
    "  * 42%\n",
    "  * 46%\n",
    "  * 50%\n",
    "  \n",
    "*Private test set* means that you won't be able to evaluate your model on it. Rather, after you submit code and checkpoint, we will load your model and evaluate it on that test set ourselves, reporting your accuracy in a comment to the grade.\n",
    "\n",
    "Note that there is an important formatting requirement, see below near \"`DO_TRAIN = True`\".\n",
    "\n",
    "## Restrictions\n",
    "\n",
    "* No pretrained networks.\n",
    "* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n",
    "\n",
    "## Tips\n",
    "\n",
    "* **One change at a time**: never test several new things at once (unless you are super confident). Train a model, introduce one change, train again.\n",
    "* Google a lot: try to reinvent as few wheels as possible (unlike in part 1 of this assignment).\n",
    "* Use GPU.\n",
    "* Use regularization: L2, batch normalization, dropout, data augmentation...\n",
    "* Pay much attention to accuracy and loss graphs (e.g. in Tensorboard). Track failures early, stop bad experiments early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m2qxvLGdFgGH",
    "outputId": "16eccde9-6b3d-4293-f48c-850c43f063bc"
   },
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2020 -- Home Assignment 2\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_6veP_5FgGS"
   },
   "source": [
    "The below cell puts training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njk50aDoFgGT"
   },
   "outputs": [],
   "source": [
    "# Extract the dataset into the current directory\n",
    "if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2020.zip', 'r') as archive:\n",
    "        archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1mtHCQcFgGb"
   },
   "source": [
    "**You are required** to format your notebook cells so that `Run All` on a fresh notebook:\n",
    "* trains your model from scratch, if `DO_TRAIN is True`;\n",
    "* loads your trained model from `\"./checkpoint.pth\"`, then **computes** and prints its validation accuracy, if `DO_TRAIN is False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QvH36LxFgGc"
   },
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rgg4D0zSFgGi"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFqnb1-EFgGj"
   },
   "outputs": [],
   "source": [
    "# Your code here (feel free to add cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TwFDnIUbvXn"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    TRAIN_DIR = './tiny-imagenet-200/train'\n",
    "VAL_DIR = './tiny-imagenet-200/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAEl1XfwbvXt"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    import os\n",
    "    CLASSES_NUMBER = len(os.listdir(TRAIN_DIR))\n",
    "    print('Number of classes: {}'.format(CLASSES_NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iOFXd9BbvXz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0) # for reproducibility\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYYCzuWmbvX4"
   },
   "source": [
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBXYuk5CbvX5"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "if DO_TRAIN:\n",
    "    augmentation_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "        torchvision.transforms.RandomAffine(10, translate=(0.1, 0.1)),\n",
    "        torchvision.transforms.ColorJitter(brightness=(0.9, 2.0), contrast=(0.9, 2.0)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=torchvision.transforms.ToTensor())\n",
    "    augmented_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, augmentation_transforms)\n",
    "    \n",
    "    train_dataset = torch.utils.data.ConcatDataset([train_dataset, augmented_dataset])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform=torchvision.transforms.ToTensor())\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc1gadkzbvX_"
   },
   "source": [
    "### Dataset Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkD_3SRrbvX_"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def show_input(ax, input_tensor, title=''):\n",
    "        image = input_tensor.permute(1, 2, 0).numpy()\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxIjwx7MbvYD"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(16, 8))\n",
    "    X_batch, y_batch = next(iter(train_dataloader))\n",
    "    \n",
    "    INPUT_SHAPE = X_batch[0].shape\n",
    "    print('Image size: {}'.format(INPUT_SHAPE))\n",
    "    for ax, x_item, y_item in zip(axes.flat, X_batch[:8], y_batch[:8]):\n",
    "        show_input(ax, x_item, title='class {}'.format(str(y_item.numpy())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdiVGqoTbvYN"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class complex_conv(torch.nn.Module):\n",
    "    def __init__(self, input_channels, mid_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(complex_2conv, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_channels, mid_channels, kernel_size, stride, padding),\n",
    "            torch.nn.BatchNorm2d(mid_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(mid_channels, output_channels, kernel_size, stride, padding),\n",
    "            torch.nn.BatchNorm2d(output_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        return output\n",
    "\n",
    "class flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(flatten, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, input_shape, classes_number):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        C, H, W = input_shape\n",
    "        \n",
    "        self.complex_conv1 = complex_conv(C, 64, 64)\n",
    "        self.complex_conv2 = complex_conv(64, 128, 128)\n",
    "        self.complex_conv3 = complex_conv(128, 256, 256)\n",
    "        self.complex_conv4 = complex_conv(256, 512, 512)\n",
    "        self.complex_conv5 = complex_conv(512, 512, 512)\n",
    "        \n",
    "        self.flatten = flatten()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(512*H*W//32//32, 1024, bias=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(1024, 512, bias=True)\n",
    "\n",
    "        self.fc3 = torch.nn.Linear(512, classes_number, bias=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.complex_conv1(input)\n",
    "        x = self.complex_conv2(x)\n",
    "        x = self.complex_conv3(x)\n",
    "        x = self.complex_conv4(x)\n",
    "        x = self.complex_conv5(x)\n",
    "        \n",
    "        flattened = self.flatten(x)\n",
    "        \n",
    "        flattened = self.fc1(flattened)\n",
    "        flattened = self.dropout(flattened)\n",
    "        flattened = self.relu(flattened)\n",
    "\n",
    "        flattened = self.fc2(flattened)\n",
    "        flattened = self.dropout(flattened)\n",
    "        flattened = self.relu(flattened)\n",
    "\n",
    "        flattened = self.fc3(flattened)\n",
    "        \n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lK8jfUeMbvYN"
   },
   "outputs": [],
   "source": [
    "# class complex_2conv(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, mid_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "#         super(complex_2conv, self).__init__()\n",
    "\n",
    "#         self.conv = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(input_channels, mid_channels, kernel_size, stride, padding),\n",
    "#             torch.nn.BatchNorm2d(mid_channels),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Conv2d(mid_channels, output_channels, kernel_size, stride, padding),\n",
    "#             torch.nn.BatchNorm2d(output_channels),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output = self.conv(input)\n",
    "#         return output\n",
    "\n",
    "# class complex_3conv(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, mid_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "#         super(complex_3conv, self).__init__()\n",
    "\n",
    "#         self.conv = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(input_channels, mid_channels, kernel_size, stride, padding),\n",
    "#             torch.nn.BatchNorm2d(mid_channels),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Conv2d(mid_channels, output_channels, kernel_size, stride, padding),\n",
    "#             torch.nn.BatchNorm2d(output_channels),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Conv2d(output_channels, output_channels, kernel_size, stride, padding),\n",
    "#             torch.nn.BatchNorm2d(output_channels),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output = self.conv(input)\n",
    "#         return output\n",
    "\n",
    "# class flatten(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(flatten, self).__init__()\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         return input.view(input.size(0), -1)\n",
    "\n",
    "# class CNN(torch.nn.Module):\n",
    "#     def __init__(self, input_shape, classes_number):\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         C, H, W = input_shape\n",
    "        \n",
    "#         self.complex_conv1 = complex_2conv(C, 64, 128)\n",
    "#         self.complex_conv2 = complex_2conv(128, 256, 256)\n",
    "#         self.complex_conv3 = complex_3conv(256, 256, 256)\n",
    "        \n",
    "#         self.flatten = flatten()\n",
    "        \n",
    "#         self.fc1 = torch.nn.Linear(256*H*W//8//8, 4096, bias=True)\n",
    "#         self.dropout = torch.nn.Dropout(p=0.2)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "\n",
    "#         self.fc2 = torch.nn.Linear(4096, 1024, bias=True)\n",
    "\n",
    "#         self.fc3 = torch.nn.Linear(1024, classes_number, bias=True)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = self.complex_conv1(input)\n",
    "#         x = self.complex_conv2(x)\n",
    "#         x = self.complex_conv3(x)\n",
    "        \n",
    "#         flattened = self.flatten(x)\n",
    "        \n",
    "#         flattened = self.fc1(flattened)\n",
    "#         flattened = self.dropout(flattened)\n",
    "#         flattened = self.relu(flattened)\n",
    "\n",
    "#         flattened = self.fc2(flattened)\n",
    "#         flattened = self.dropout(flattened)\n",
    "#         flattened = self.relu(flattened)\n",
    "\n",
    "#         flattened = self.fc3(flattened)\n",
    "        \n",
    "#         return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1SyF33mAn4B"
   },
   "outputs": [],
   "source": [
    "# class BottleneckBlock(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "#         super(BottleneckBlock, self).__init__()\n",
    "        \n",
    "#         mid_channels = input_channels*2\n",
    "#         self.bn1 = torch.nn.BatchNorm2d(input_channels)\n",
    "#         self.relu = torch.nn.ReLU(inplace=True)\n",
    "#         self.conv1 = torch.nn.Conv2d(input_channels, mid_channels, kernel_size, stride, padding)\n",
    "\n",
    "#         self.bn2 = torch.nn.BatchNorm2d(mid_channels)\n",
    "#         self.conv2 = torch.nn.Conv2d(mid_channels, output_channels, kernel_size, stride, padding)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output = self.conv1(self.relu(self.bn1(input)))\n",
    "#         output = self.conv2(self.relu(self.bn2(output)))\n",
    "        \n",
    "#         return torch.cat([input, output], dim=1)\n",
    "\n",
    "# class BasicBlock(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "\n",
    "#         self.bn = torch.nn.BatchNorm2d(input_channels)\n",
    "#         self.relu = torch.nn.ReLU(inplace=True)\n",
    "#         self.conv = torch.nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         output = self.conv(self.relu(self.bn(input)))\n",
    "#         return torch.cat([input, output], dim=1)\n",
    "\n",
    "# class DenseBlock(torch.nn.Module):\n",
    "#     def __init__(self, layers_number, block, input_channels, growth_rate):\n",
    "#         super(DenseBlock, self).__init__()\n",
    "\n",
    "#         self.layer = self.concat_layers(layers_number, block, input_channels, growth_rate)\n",
    "    \n",
    "#     def concat_layers(self, layers_number, block, input_channels, growth_rate):\n",
    "#         layers = []\n",
    "#         for i in range(layers_number):\n",
    "#             layers.append(block(input_channels + i*growth_rate, growth_rate))\n",
    "#         return torch.nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.layer(input)\n",
    "\n",
    "# class TransitionBlock(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, padding=1):\n",
    "#         super(TransitionBlock, self).__init__()\n",
    "\n",
    "#         self.bn1 = torch.nn.BatchNorm2d(input_channels)\n",
    "#         self.relu = torch.nn.ReLU(inplace=True)\n",
    "#         self.conv1 = torch.nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding)\n",
    "#         self.pool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         output = self.conv1(self.relu(self.bn1(input)))\n",
    "#         return self.pool(output)\n",
    "\n",
    "# class DenseNet(torch.nn.Module):\n",
    "#     def __init__(self, input_shape, classes_number, growth_rate=24, reduction=0.8):\n",
    "#         super(DenseNet, self).__init__()\n",
    "\n",
    "#         C, H, W = input_shape\n",
    "#         in_channels = 2*growth_rate\n",
    "#         layers_number = 3\n",
    "\n",
    "#         self.conv1 = torch.nn.Conv2d(C, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         self.block1 = DenseBlock(layers_number, BasicBlock, in_channels, growth_rate)\n",
    "#         in_channels += layers_number*growth_rate\n",
    "#         self.transition1 = TransitionBlock(in_channels, int(in_channels*reduction))\n",
    "#         in_channels = int(in_channels*reduction)\n",
    "\n",
    "#         self.block2 = DenseBlock(layers_number, BasicBlock, in_channels, growth_rate)\n",
    "#         in_channels += layers_number*growth_rate\n",
    "#         self.transition2 = TransitionBlock(in_channels, int(in_channels*reduction))\n",
    "#         in_channels = int(in_channels*reduction)\n",
    "\n",
    "#         self.block3 = DenseBlock(layers_number, BasicBlock, in_channels, growth_rate)\n",
    "#         in_channels += layers_number*growth_rate\n",
    "#         self.in_channels = in_channels\n",
    "\n",
    "#         self.bn = torch.nn.BatchNorm2d(in_channels)\n",
    "#         self.relu = torch.nn.ReLU(inplace=True)\n",
    "#         self.pool = torch.nn.AvgPool2d(kernel_size=H//2//2)\n",
    "#         self.fc = torch.nn.Linear(in_channels, classes_number)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         output = self.conv1(input)\n",
    "\n",
    "#         output = self.transition1(self.block1(output))\n",
    "#         output = self.transition2(self.block2(output))\n",
    "\n",
    "#         output = self.block3(output)\n",
    "#         output = self.relu(self.bn(output))\n",
    "\n",
    "#         output = self.pool(output)\n",
    "#         output = output.view(-1, self.in_channels)\n",
    "\n",
    "#         return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2M_WJwAybvYQ",
    "outputId": "26cf46cc-70b2-40dd-d191-c955d321ccee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: {}'.format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xemsTW4SbvYU"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    from torchsummary import summary\n",
    "\n",
    "    model = CNN(INPUT_SHAPE, CLASSES_NUMBER)\n",
    "    print('Model summary:')\n",
    "    summary(model.to(DEVICE), INPUT_SHAPE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfoUPr1vbvYZ"
   },
   "source": [
    "### Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9VU4R-JbvYa"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    LEARNING_RATE = 1.0e-3\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Km0FkrFobvYk"
   },
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDxQHL30bvYm"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if DO_TRAIN:\n",
    "    def accuracy_evaluating(predictions, labels):\n",
    "        logits = torch.nn.LogSoftmax(dim=1)(predictions)\n",
    "        prediction_labels = logits.max(dim=1).indices\n",
    "\n",
    "        return (labels == prediction_labels).float().mean()\n",
    "\n",
    "if not DO_TRAIN:\n",
    "    def accuracy(model, dataloader):\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        running_accuracy = 0.0\n",
    "        print('accuracy evaluating...')\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)  \n",
    "\n",
    "            predictions = model(images)\n",
    "            logits = torch.nn.LogSoftmax(dim=1)(predictions)\n",
    "            prediction_labels = logits.max(dim=1).indices\n",
    "\n",
    "            running_accuracy += (labels == prediction_labels).float().mean()\n",
    "        \n",
    "        return running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38Hogc5YbvYc"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUZOA7p9sT41"
   },
   "outputs": [],
   "source": [
    "# TensorBoard interactive\n",
    "if DO_TRAIN:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    LOG_DIR = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRZp4tk1bvYe"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    def train(model, train_dataloader, val_dataloader,\n",
    "              criterion,\n",
    "              optimizer, scheduler,\n",
    "              epochs,\n",
    "              device,\n",
    "              experiment_name):\n",
    "        writer = SummaryWriter('{}/{}'.format(LOG_DIR, experiment_name))\n",
    "        model.to(device)\n",
    "\n",
    "        print('Train model for {} epochs\\n'.format(epochs))\n",
    "        for epoch in range(12, epochs + 12):\n",
    "            print('Epoch {} train stage...'.format(epoch))\n",
    "            model.train()\n",
    "            train_running_loss = 0.0\n",
    "            train_running_accuracy = 0.0\n",
    "            \n",
    "            for images, labels in tqdm(train_dataloader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(images)\n",
    "                \n",
    "                train_loss = criterion(predictions, labels)\n",
    "                train_running_loss += train_loss.item()\n",
    "\n",
    "                train_running_accuracy += accuracy_evaluating(predictions,\n",
    "                                                              labels)\n",
    "                \n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            writer.add_scalar('train loss',\n",
    "                              train_running_loss / len(train_dataloader),\n",
    "                              epoch)\n",
    "            \n",
    "            writer.add_scalar('train accuracy',\n",
    "                              train_running_accuracy / len(train_dataloader),\n",
    "                              epoch)\n",
    "            \n",
    "            print('Epoch {} validation stage...'.format(epoch))\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_running_accuracy = 0.0\n",
    "            \n",
    "            for images, labels in tqdm(val_dataloader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                predictions = model(images)\n",
    "                \n",
    "                val_loss = criterion(predictions, labels)\n",
    "                val_running_loss += val_loss.item()\n",
    "\n",
    "                val_running_accuracy += accuracy_evaluating(predictions,\n",
    "                                                            labels)\n",
    "            \n",
    "            writer.add_scalar('val loss',\n",
    "                              val_running_loss / len(val_dataloader),\n",
    "                              epoch)\n",
    "            \n",
    "            writer.add_scalar('val accuracy',\n",
    "                              val_running_accuracy / len(val_dataloader),\n",
    "                              epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9n7DyGcFgGq"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    # Your code here (train your model)\n",
    "    # etc.\n",
    "    EPOCHS = 2\n",
    "    \n",
    "    train(model, train_dataloader, val_dataloader,\n",
    "          criterion,\n",
    "          optimizer, scheduler,\n",
    "          epochs=EPOCHS,\n",
    "          device=DEVICE,\n",
    "          experiment_name='CNN_3_{}_{}'.format(BATCH_SIZE, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zR2L4zSCjhk7"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbjVLPByjgxP"
   },
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./checkpoint.pth\"\n",
    "if DO_TRAIN:\n",
    "    torch.save(model, PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHFdDYv8FgGx"
   },
   "source": [
    "## Load and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-YNjqEMFgGy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here (load the model from \"./checkpoint.pth\")\n",
    "# Please use `torch.load(\"checkpoint.pth\", map_location='cpu')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu9weCI7ktlB"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZKJ6vbWkv_q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "DO_TRAIN = False\n",
    "if not DO_TRAIN:\n",
    "    model = torch.load(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "1209a6689fc942ee8eaa462e10742916",
      "fdeff8c25bc14493b2da4fdd88696c22",
      "0f02852d51e948c8a3672fbca4d4bb87",
      "44110d2751f749708585ed87cfae3f77",
      "a40b7738b7b640408f43f3c333e24f6a",
      "07f20ec6135a435db5c569227f9104dc",
      "588ba12160324a4db1ee236bb08eddff",
      "e96d5c9f790245cca60205e40b9477d3"
     ]
    },
    "colab_type": "code",
    "id": "VYKCk2rnFgG2",
    "outputId": "568e20fc-43ff-4187-d994-c86ec89177f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d3ee310e064a0ca3530c3785d5f1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 44.73%\n"
     ]
    }
   ],
   "source": [
    "if not DO_TRAIN:\n",
    "    val_accuracy = 100*accuracy(model, val_dataloader)\n",
    "    assert 0 <= val_accuracy <= 100\n",
    "    print(\"Validation accuracy: %.2f%%\" % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuY9IgkY-N7y"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsxZDrLqOyl4"
   },
   "source": [
    "Via **TensorBoard** interactive I obtained the follows plots:\n",
    "\n",
    "On all plots, the $\\color{orange}{\\bf{orange}}$ line corresponds to the penultimate model, and the $\\color{blue}{\\bf{blue}}$ line corresponds to the last model.\n",
    "\n",
    "**Train loss**\n",
    "![train loss](./train_loss.svg)\n",
    "\n",
    "**Train accuracy**\n",
    "![train accuracy](./train_accuracy.svg)\n",
    "\n",
    "**Validation loss**\n",
    "![val loss](./val_loss.svg)\n",
    "\n",
    "**Validation accuracy**\n",
    "![val accuracy](./val_accuracy.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crMQ_SvcFgG7"
   },
   "source": [
    "# Report\n",
    "\n",
    "Below, please mention:\n",
    "\n",
    "* A brief history of tweaks and improvements.\n",
    "* Which network architectures have you tried? What is the final one and why?\n",
    "* What is the training method (batch size, optimization algorithm, number of iterations, ...) and why?\n",
    "* Which techniques have you tried to prevent overfitting? What were their effects? Which of them worked well?\n",
    "* Any other insights you learned.\n",
    "\n",
    "For example, start with:\n",
    "\n",
    "\"I have analyzed these and those conference papers/sources/blog posts. \\\n",
    "I tried this and that to adapt them to my problem. \\\n",
    "The conclusions this task taught me are ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBcPj7EyFgG8"
   },
   "source": [
    "###### [Dataset Preprocessing](#Dataset-Download)\n",
    "\n",
    "At first, I downloaded the data and looked at the images. Data preprocessing is one of the most important parts in Machine Learning, and in particular, in Computer Vision. For better performance of the model and prevent overfitting I will use the **augmentation** method $\\color{blue}{[1, 2]}$. Of course, augmentation should be reasonable. In this case I use the following types:\n",
    "\n",
    "* Horizontal axis ï¬‚ipping\n",
    "* Slight rotation\n",
    "* Slight translation\n",
    "* Color space transformations\n",
    "\n",
    "###### [CNN Architecture](#Model)\n",
    "\n",
    "As a baseline, I took a mixture of of lightweight (with fewer parameters) classical CNN architectures: LeNet $\\color{blue}{[3]}$, AlexNet $\\color{blue}{[4]}$ and VGGNet $\\color{blue}{[5]}$. In this model I added the following modules used in modern Deep Learning:\n",
    "\n",
    "* **Batch Normalization**. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. Applied to a state-of-the-art image classification model $\\color{blue}{[6]}$.\n",
    "\n",
    "* **ReLU**. The rectified linear unit (ReLU) activation function is the most widely used activation function for deep learning applications with state-of-the-art results to date $\\color{blue}{[7]}$. The ReLU represents a nearly linear function and therefore preserves the properties of linear models that made them easy to optimize, with gradient-descent method.\n",
    "\n",
    "Thus, the following model was obtained:\n",
    "\n",
    "**Model**: `complex_conv ==> complex_conv ==> Flatten ==> FC ==> FC ==> FC`,\n",
    "\n",
    "where `complex_conv = (Conv3x3 ==> BatchNorm ==> ReLU ==> Conv3x3 ==> BatchNorm ==> ReLU ==> MaxPool2x2)`.\n",
    "\n",
    "###### [Model Training](#Training-Method)\n",
    "\n",
    "To train the model, I have to use some loss function and optimization method. I chose:\n",
    "\n",
    "* **Cross Entropy Loss**. The softmax function is widely adopted by many CNNs due to its simplicity and probabilistic interpretation. Together with the cross-entropy loss, they form arguably one of the most commonly used components in CNN architectures $\\color{blue}{[8]}$.\n",
    "\n",
    "* **ADAM**. Adam is algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods $\\color{blue}{[9]}$.\n",
    "\n",
    "As for any stochastic gradient descent method (including the mini-batch case), it is important for efficiency of the estimator that each example or minibatch be sampled approximately independently. In this context, it is safer if the examples or mini-batches are first put in a random order ([**shuffled**](#Dataset-Download)) $\\color{blue}{[10]}$.\n",
    "\n",
    "###### [Model Evaluation](#Metric)\n",
    "\n",
    "To evaluate the performance of the model, I will use the **accuracy** metric (required by this assignment).\n",
    "\n",
    "###### [Learning Process](#Model-Training)\n",
    "\n",
    "When you want to train a model, **do not forget to set your paths!**\n",
    "\n",
    "* [LOG_DIR](#Model-Training)\n",
    "* [PATH_TO_MODEL](#Save-Model)\n",
    "\n",
    "To train the model I set the following **parameters**:\n",
    "\n",
    "* [Batch size](#Dataset-Download) = 100\n",
    "* [Learning Rate](#Training-Method) = 0.001\n",
    "* [Epochs number](#Model-Training) = 20\n",
    "\n",
    "The following best **results** were obtained with the baseline model:\n",
    "\n",
    "* train phase:\n",
    "    * loss value: 1.79\n",
    "    * accuracy: 0.567\n",
    "* validation phase:\n",
    "    * loss value: 2.9\n",
    "    * accuracy: 0.35\n",
    "\n",
    "After that I decided to improve my model.\n",
    "\n",
    "###### [Model Improvings](#Model)\n",
    "\n",
    "1. At first I decided to try a completely different architecture - some improvement of ResNet - **DenseNet** $\\color{blue}{[11]}$. Of course, I used a somewhat lightweight network, which in spite of this required a lot of resources and a long training time. The following best **results** were obtained with this model:\n",
    "\n",
    "  * train phase:\n",
    "      * loss value: 1.12\n",
    "      * accuracy: 0.695\n",
    "  * validation phase:\n",
    "      * loss value: 2.305\n",
    "      * accuracy: 0.457\n",
    "\n",
    "2. Then I decided improve baseline model. I used: \n",
    "\n",
    "  * **dropout** technique for regularization and preventing the co-adaptation of neurons $\\color{blue}{[12]}$,\n",
    "  * activation functions after fully connected layers,\n",
    "  * another three convolutional layers.\n",
    "\n",
    "  Thus, I got a model that has the same skeleton as **VGGNet-B**:\n",
    "\n",
    "  **Model**: `complex_conv ==> complex_conv ==> complex_conv ==> complex_conv ==> complex_conv ==> Flatten ==> FC => Dropout ==> ReLU ==> FC ==> Dropout ==> ReLU ==> FC`,\n",
    "\n",
    "  where `complex_conv = (Conv3x3 ==> BatchNorm ==> ReLU ==> Conv3x3 ==> BatchNorm ==> ReLU ==> MaxPool2x2)`.\n",
    "\n",
    "  The following best **results** were obtained with this model:\n",
    "\n",
    "  * train phase:\n",
    "      * loss value: 1.97\n",
    "      * accuracy: 0.51\n",
    "  * validation phase:\n",
    "      * loss value: 2.211\n",
    "      * accuracy: 0.483\n",
    "  \n",
    "  In this case, I got the highest accuracy and the lowest value of the loss function, but there was an overfitting (the loss function of validation began to increase monotonously, starting from the 14th epoch). \n",
    "  \n",
    "  Since this model showed the best results in validation, and also turned out to be lighter (parameters number) and faster on training than the previous one, I tried to overcome overfitting using the **learning rate scheduler** $\\color{blue}{[13]}$:\n",
    "\n",
    "  * [Scheduler](#Training-Method): step size = 10, gamma = 0.1\n",
    "  \n",
    "  The point is that if at some moment on training we fall on a plateau, then reducing the step of the gradient descent, we can find a gap that will lead to a smaller local minimum. \n",
    "  \n",
    "Next, I tried two different versions for improving:\n",
    "\n",
    "3. For the first one I also reduced the batch size:\n",
    "  \n",
    "  * [Batch size](#Dataset-Download) = 32\n",
    "  \n",
    "  Smaller values of batch size may benefit from more exploration in parameter space and a form of regularization both due to the \"noise\" injected in the gradient estimator, which may explain the better test results sometimes observed with smaller batch size $\\color{blue}{[10]}$.\n",
    "\n",
    "  The following best **results** were obtained with this model:\n",
    "\n",
    "  * train phase:\n",
    "      * loss value: 1.944\n",
    "      * accuracy: 0.503\n",
    "  * validation phase:\n",
    "      * loss value: 2.282\n",
    "      * accuracy: 0.447\n",
    "\n",
    "4. For the second one I changed the architecture (but again considered VGGNet-like) and increased feature maps on first convolutions:\n",
    "\n",
    "  **Model**: `complex_2conv ==> complex_2conv ==> complex_3conv ==> Flatten ==> FC => Dropout ==> ReLU ==> FC ==> Dropout ==> ReLU ==> FC`,\n",
    "\n",
    "  where `complex_2conv = (Conv3x3 ==> BatchNorm ==> ReLU ==> Conv3x3 ==> BatchNorm ==> ReLU ==> MaxPool2x2)` and `complex_3conv = (Conv3x3 ==> BatchNorm ==> ReLU ==> Conv3x3 ==> BatchNorm ==> ReLU ==> Conv3x3 ==> BatchNorm ==> ReLU ==> MaxPool2x2)`\n",
    "\n",
    "  The following best **results** were obtained with this model (**launched on 12 epochs**):\n",
    "  \n",
    "  * train phase:\n",
    "      * loss value: 1.82\n",
    "      * accuracy: 0.533\n",
    "  * validation phase:\n",
    "      * loss value: 2.234\n",
    "      * accuracy: 0.459\n",
    "\n",
    "###### [Results](#Results)\n",
    "\n",
    "Via **TensorBoard** interactive I obtained the follows plots:\n",
    "\n",
    "* train accuracy\n",
    "* train loss\n",
    "* val accuracy\n",
    "* val loss\n",
    "\n",
    "Then I downloaded the plots in svg format and insert it into the notebook.\n",
    "\n",
    "###### [References](#Report):\n",
    "\n",
    "$\\color{blue}{[1]}$ Shorten, Connor & Khoshgoftaar, Taghi. (2019). [A survey on Image Data Augmentation for Deep Learning](https://www.researchgate.net/publication/334279066_A_survey_on_Image_Data_Augmentation_for_Deep_Learning). Journal of Big Data. 6. 10.1186/s40537-019-0197-0.\n",
    "\n",
    "$\\color{blue}{[2]}$ Perez, Luis & Wang, Jason. (2017). [The Effectiveness of Data Augmentation in Image Classification using Deep Learning](https://arxiv.org/pdf/1712.04621.pdf).\n",
    "\n",
    "$\\color{blue}{[3]}$ Lecun, Yann & Bottou, Leon & Bengio, Y. & Haffner, Patrick. (1998). [Gradient-Based Learning Applied to Document Recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). Proceedings of the IEEE. 86. 2278 - 2324. 10.1109/5.726791.\n",
    "\n",
    "$\\color{blue}{[4]}$ Krizhevsky, Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012). [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). Neural Information Processing Systems. 25. 10.1145/3065386.\n",
    "\n",
    "$\\color{blue}{[5]}$ Simonyan, Karen & Zisserman, Andrew. (2014). [Very Deep Convolutional Networks for Large-Scale Image Recognition](http://www.robots.ox.ac.uk/~vgg/publications/2015/Simonyan15/simonyan15.pdf). arXiv 1409.1556.\n",
    "\n",
    "$\\color{blue}{[6]}$ Ioffe, Sergey & Szegedy, Christian. (2015). [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf).\n",
    "\n",
    "$\\color{blue}{[7]}$ Hinton, Geoffrey. (2010). [Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf). Proceedings of ICML. 27. 807-814.\n",
    "\n",
    "$\\color{blue}{[8]}$ Liu, Weiyang & Wen, Yandong & Yu, Zhiding & Yang, Meng. (2016). [Large-Margin Softmax Loss for Convolutional Neural Networks](https://arxiv.org/pdf/1612.02295.pdf). ProC. Int. Conf. Mach. Learn.\n",
    "\n",
    "$\\color{blue}{[9]}$ Kingma, Diederik & Ba, Jimmy. (2014). [Adam: A Method for Stochastic Optimization. International Conference on Learning Representations](https://arxiv.org/pdf/1412.6980.pdf).\n",
    "\n",
    "$\\color{blue}{[10]}$ Bengio, Y.. (2012). [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/pdf/1206.5533v2.pdf). Arxiv. \n",
    "\n",
    "$\\color{blue}{[11]}$ Huang, Gao & Liu, Zhuang & van der Maaten, Laurens & Weinberger, Kilian. (2017). [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf). 10.1109/CVPR.2017.243.\n",
    "\n",
    "$\\color{blue}{[12]}$ Srivastava, Nitish & Hinton, Geoffrey & Krizhevsky, Alex & Sutskever, Ilya & Salakhutdinov, Ruslan. (2014). [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). Journal of Machine Learning Research. 15. 1929-1958.\n",
    "\n",
    "$\\color{blue}{[13]}$ Darken, Christian & Moody, John. (1990). [Note on Learning Rate Schedules for Stochastic Optimization](https://pdfs.semanticscholar.org/713f/55820406c9540428ae5ec2a0428010d6800c.pdf). 832-838."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07f20ec6135a435db5c569227f9104dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f02852d51e948c8a3672fbca4d4bb87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07f20ec6135a435db5c569227f9104dc",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a40b7738b7b640408f43f3c333e24f6a",
      "value": 100
     }
    },
    "1209a6689fc942ee8eaa462e10742916": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f02852d51e948c8a3672fbca4d4bb87",
       "IPY_MODEL_44110d2751f749708585ed87cfae3f77"
      ],
      "layout": "IPY_MODEL_fdeff8c25bc14493b2da4fdd88696c22"
     }
    },
    "44110d2751f749708585ed87cfae3f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e96d5c9f790245cca60205e40b9477d3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_588ba12160324a4db1ee236bb08eddff",
      "value": " 100/100 [00:05&lt;00:00, 17.18it/s]"
     }
    },
    "588ba12160324a4db1ee236bb08eddff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a40b7738b7b640408f43f3c333e24f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e96d5c9f790245cca60205e40b9477d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdeff8c25bc14493b2da4fdd88696c22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
